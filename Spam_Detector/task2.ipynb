{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d650ba7-9ce8-42ac-af2a-106a7ca4f6a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Spam Mail Detector**\n",
    "\n",
    "---\n",
    "\n",
    "### **Objective:** Build a classifier that distinguishes between spam and non-spam (ham) emails using textual data. \n",
    "\n",
    "### **Dataset:** Public datasets like the SMS Spam Collection (UCI) or Enron Email Dataset. \n",
    "\n",
    "---\n",
    "\n",
    "**1. Load the messages and labels (spam or ham).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0ed5f2-acaf-4239-9b2b-a37ae2721f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568   ham               Will Ã¼ b going to esplanade fr home?\n",
      "5569   ham  Pity, * was in mood for that. So...any other s...\n",
      "5570   ham  The guy did some bitching but I acted like i'd...\n",
      "5571   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"label\", \"message\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38f8029f-e1af-42f4-933c-f2e6822b1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eada44-a994-45c0-b015-752dc63e9102",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2. Preprocess the text (lowercasing, remove stopwords, tokenization).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae6158b1-4927-495a-b475-38456758bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76460104-ea7a-47b6-84a0-803da8886816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download stopwords & punkt tokenizer\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d61bfc6a-23f9-41e6-bc53-ff2e942f5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f478ed9d-8518-4be1-a1d6-9527b39f846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                       clean_message  \n",
      "0  go jurong point , crazy .. available bugis n g...  \n",
      "1                    ok lar ... joking wif u oni ...  \n",
      "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
      "3        u dun say early hor ... u c already say ...  \n",
      "4       nah n't think goes usf , lives around though  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    #1. Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    #2. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #3. Remove Stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #4. join tokens back to a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to messages\n",
    "df[\"clean_message\"] = df[\"message\"].apply(preprocess_text)\n",
    "\n",
    "print(df[[\"message\", \"clean_message\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010205f-3ffe-4f08-8b82-54954c0efa1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**3. Convert text into numeric features (Bag of Words or TF-IDF).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77cf7b39-4f0b-45b3-8177-3310393eb967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BoW matrix: (5572, 8645)\n",
      "Example feature names: ['00' '000' '000pes' '008704050406' '0089' '0121' '01223585236'\n",
      " '01223585334' '0125698789' '02' '0207' '02072069400' '02073162414'\n",
      " '02085076972' '021' '03' '04' '0430' '05' '050703']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vect = CountVectorizer()\n",
    "\n",
    "x_bow = bow_vect.fit_transform(df[\"clean_message\"])\n",
    "\n",
    "print(\"Shape of BoW matrix:\", x_bow.shape)\n",
    "print(\"Example feature names:\", bow_vect.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e105f6a-a563-4c80-831c-9814dba1af4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**4. Split into train/test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa34f616-e476-4840-b42d-103e0c773cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4457, 8645)\n",
      "Test set shape: (1115, 8645)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = x_bow\n",
    "y = df[\"label\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Test set shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611246aa-5d7f-4b21-b9ac-bd027b834700",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**5. Train a simple model (Naive Bayes, Logistic Regression).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "255fb039-062f-413d-8f90-313cdcdf972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train, y_train)\n",
    "y_pred_nb = nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed365a66-ed02-46ee-a09a-8d2cea6f386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logictic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l_reg = LogisticRegression()\n",
    "\n",
    "l_reg.fit(x_train, y_train)\n",
    "y_pred_l_reg = l_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0d343-cb84-4c4f-9ede-5ca42ab23dde",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**6. Measure performance with accuracy, precision, or F1 score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4118cd12-1467-4b3c-bb80-a7ed026c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96e3fe3d-4e1f-44a3-b7b8-61cf2beda384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979372197309417\n",
      "Precision: 0.9801305485955697\n",
      "F1 Score: 0.9796262882489709\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes model performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_nb, average=\"weighted\"))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_nb, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fd71178-27c0-4388-9a8a-212cab33ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856502242152466\n",
      "Precision: 0.9858840291160164\n",
      "F1 score: 0.9853020696947724\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_l_reg))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_l_reg, average=\"weighted\"))\n",
    "print(\"F1 score:\", f1_score(y_test, y_pred_l_reg, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e115ed-dec4-4200-8082-5f8954cf8c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
